{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ace58e3-d3d7-4743-a14c-21a4f6e2aa21",
   "metadata": {},
   "source": [
    "# Module 8 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f461dda1",
   "metadata": {},
   "source": [
    "Today's data set consists of time series data measured at 10 min intervals for about 4.5 months. A house temperature and humidity conditions were monitored with a ZigBee wireless sensor network. Each wireless node transmitted the temperature and humidity conditions every 3.3 min. Then, the wireless data was averaged for 10 minutes periods. The energy data was logged every 10 minutes with m-bus energy meters. Weather from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis (rp5.ru), and merged together with the experimental data sets using the date and time column. Two random variables have been included in the data set for testing the regression models and to filter out non predictive attributes (parameters). We will use this data to predict the current energy usage of the appliances in the home based on the indoor and outdoor predictors. Notice, this is not a forecasting excercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103cd06",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's get all the requirements sorted before we move on to the excercise. Notice, today we will be using the datetime package to deal with timestamps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "!pip install --upgrade ipykernel\n",
    "!pip install datetime\n",
    "!pip install pandas\n",
    "!pip install tableone\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install boruta\n",
    "!pip install sklearn\n",
    "\n",
    "# Globals\n",
    "seed = 1017\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tableone import TableOne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#magic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bd5b1-61c0-435a-b1ae-3047d3519bae",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The data for today can be found in the `data` folder distributed along with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d23b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data as a pandas dataframe\n",
    "df = pd.read_csv(\"data/KAG_energydata_complete.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301be4c",
   "metadata": {},
   "source": [
    "## Data description\n",
    "|column | description |\n",
    "|-------|-------------|\n",
    "|date | time year-month-day hour:minute:second|\n",
    "|Appliances | energy use in Wh |\n",
    "|lights| energy use of light fixtures in the house in Wh |\n",
    "|T1| Temperature in kitchen area, in Celsius|\n",
    "|RH1| Relative humidity in kitchen area, in %|\n",
    "|T2| Temperature in living room area, in Celsius |\n",
    "|RH2| Relative humidity in living room area, in % |\n",
    "|T3| Temperature in laundry room area, in Celsius |\n",
    "|RH3| Humidity in laundry room area, in % |\n",
    "|T4| Temperature in office room, in Celsius|\n",
    "|RH4| Humidity in office room, in %|\n",
    "|T5| Temperature in bathroom, in Celsius|\n",
    "|RH5| Humidity in bathroom, in % |\n",
    "|T6| Temperature outside the building (north side), in Celsius |\n",
    "|RH6| Humidity outside the building (north side), in %|\n",
    "|T7| Temperature in ironing room, in Celsius|\n",
    "|RH7| Humidity in ironing room, in % |\n",
    "|T8| Temperature in teenager room 2, in Celsius |\n",
    "|RH8| Humidity in teenager room 2, in %|\n",
    "|T9| Temperature in parents room, in Celsius|\n",
    "|RH9| Humidity in parents room, in % |\n",
    "|Tout| Temperature outside (from Chievres weather station), in Celsius|\n",
    "|Press_mm_hg| Barometric Pressure at Chievres weather station, in mm Hg |\n",
    "|RHout| Humidity outside (from Chievres weather station), in %|\n",
    "|Windspeed| Wind speed at Chievres weather station, in m/s|\n",
    "|Visibility| Ground visibility at Chievres weather station, in km|\n",
    "|Tdewpoint| Condensation Temperature at Chievres weather station, in Celsius|\n",
    "|rv1| Random variable 1, nondimensional|\n",
    "|rv2| Random variable 2, nondimensional|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60583afe",
   "metadata": {},
   "source": [
    "## Formatting\n",
    "As is, the date column is acting like a bookeeping index for each observation. Maybe we can get some useful perdictors out of it. Let's engineer a time of day numeric feauture and a weekend/weekday binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert strings in date column into datetime objects\n",
    "datetimelist = [ datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\") for date in df['date'] ]\n",
    "\n",
    "#extract time of day in minutes\n",
    "df['time'] = [ obs.hour * 24.0 + obs.minute for obs in datetimelist ]\n",
    "\n",
    "#extract the day of week as integer 0-6 for Monday-Sunday\n",
    "df['day'] = [ obs.weekday() for obs in datetimelist ]\n",
    "\n",
    "#remove the date column\n",
    "df.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bin appliance energy usage into above and below the median to comapre features\n",
    "df['bin'] = df['Appliances']>np.median(df['Appliances'])\n",
    "\n",
    "# Generate table 1 - group by the Appliance energy use bins\n",
    "tbl1 = TableOne(df, groupby='bin', categorical=['day', 'lights'], \n",
    "                pval=True,\n",
    "                dip_test=True,\n",
    "                normal_test=True,\n",
    "                tukey_test=True)\n",
    "\n",
    "#Remove the bin variable we created for diagnostic puroposes\n",
    "df.drop('bin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the table 1\n",
    "display(tbl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d5a51",
   "metadata": {},
   "source": [
    "### Feature distributions\n",
    "Let's plot the feature distributions and see if we can address the warnings raised by the table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75a9a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot the feature distributions\n",
    "for feat in df.columns: \n",
    "    df[[feat]].dropna().plot.kde(bw_method='scott') #use bw_method=.02 for a lower bandwidth gaussian representation\n",
    "    plt.legend([feat])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f634a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute any missing values with their column median\n",
    "df.fillna(value=df.median(axis=1, skipna=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log2 transform - you will need to identify any features with long tails\n",
    "cols = ['Appliances', 'lights']\n",
    "df[cols] = np.log(df[cols]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa49661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean center and unit scale the standard deviation for the numerical variables\n",
    "cols = df.columns[~df.columns.isin(['day','lights'])]\n",
    "df[cols] = stats.zscore(df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#70-30 partition\n",
    "df_test = df.sample(frac=0.3)\n",
    "df_train = df.drop(df_test.index)\n",
    "display(df_train.shape)\n",
    "display(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3269faa",
   "metadata": {},
   "source": [
    "## Linear Regression \n",
    "Let's explore linear regression with regularization using the ridge, lasso, and elasticnt regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded7467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# get predictors and labels\n",
    "X = np.array(df_train.drop('Appliances', axis=1)) \n",
    "y = np.array(df_train['Appliances'])\n",
    "\n",
    "\n",
    "#train train ridge regression model with 10-fold cross validataion\n",
    "ridge = RidgeCV(cv=5).fit(X,y)\n",
    "\n",
    "#plot feature importance based on coeficients\n",
    "importance = np.abs(ridge.coef_)\n",
    "feature_names = np.array(df.columns.drop('Appliances'))\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()\n",
    "\n",
    "# Get model score in the testing set\n",
    "X_test = np.array(df_test.drop('Appliances', axis=1)) \n",
    "y_test = np.array(df_test['Appliances'])\n",
    "\n",
    "#display the model score\n",
    "display(ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# get predictors and labels\n",
    "X = np.array(df_train.drop('Appliances', axis=1)) \n",
    "y = np.array(df_train['Appliances'])\n",
    "\n",
    "#train lasso model with 10-fold cross validataion\n",
    "lasso = LassoCV(max_iter=1000, tol=0.0001, cv=5,\n",
    "                verbose=0, n_jobs=-1, random_state=seed, selection='random').fit(X,y)\n",
    "\n",
    "#display the model score\n",
    "display(lasso.score(X, y))\n",
    "\n",
    "#plot feature importance based on coeficients\n",
    "importance = np.abs(lasso.coef_)\n",
    "feature_names = np.array(df.columns.drop('Appliances'))\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()\n",
    "\n",
    "# Get model score in the testing set\n",
    "X_test = np.array(df_test.drop('Appliances', axis=1)) \n",
    "y_test = np.array(df_test['Appliances'])\n",
    "\n",
    "#display the model score\n",
    "display(lasso.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# get predictors and labels\n",
    "X = np.array(df_train.drop('Appliances', axis=1)) \n",
    "y = np.array(df_train['Appliances'])\n",
    "\n",
    "#train lasso model with 10-fold cross validataion\n",
    "elastic = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], max_iter=1000, tol=0.0001, cv=5,\n",
    "                     verbose=0, n_jobs=-1, random_state=seed, selection='random').fit(X,y)\n",
    "\n",
    "#display the model score\n",
    "display(elastic.score(X, y))\n",
    "\n",
    "#plot feature importance based on coeficients\n",
    "importance = np.abs(elastic.coef_)\n",
    "feature_names = np.array(df.columns.drop('Appliances'))\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()\n",
    "\n",
    "# Get model score in the testing set\n",
    "X_test = np.array(df_test.drop('Appliances', axis=1)) \n",
    "y_test = np.array(df_test['Appliances'])\n",
    "\n",
    "#display the model score\n",
    "display(elastic.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d5cb7",
   "metadata": {},
   "source": [
    "## Non-Linear Regression\n",
    "Let's explore non-linear regression with a multi layer perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# get predictors and labels\n",
    "X = np.array(df_train.drop('Appliances', axis=1)) \n",
    "y = np.array(df_train['Appliances'])\n",
    "\n",
    "#train AdaBoost model \n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10,10), activation='relu',\n",
    "                        solver='adam', alpha=0.0001, batch_size='auto',\n",
    "                        learning_rate='constant', learning_rate_init=0.001,\n",
    "                        max_iter=200, shuffle=True, random_state=seed, tol=0.0001,\n",
    "                        verbose=False, warm_start=False, \n",
    "                        early_stopping=True, validation_fraction=0.1,\n",
    "                        beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10).fit(X,y)\n",
    "\n",
    "# Get model score in the testing set\n",
    "X_test = np.array(df_test.drop('Appliances', axis=1)) \n",
    "y_test = np.array(df_test['Appliances'])\n",
    "\n",
    "#display the model score\n",
    "display(mlp.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac95fb7",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "### Loading the data via kaggle API\n",
    "The data set we used today was sourced from kaggle. Although one can manually download data this way, it is much more efficient and safer to aquire your source data programmatically. \n",
    "To download the data directly from [kaggle](kaggle.com) you will need to have a kaggle account. **It's free.** Once you create your kaggle account you can generate an API token. After you log in you should see a circular account icon in the upper-right of any kaggle page. Clicking on your account icon will open a right-sidebar where you can select \"Account\" to edit your account. Scroll down to the API section and click on the \"create new api token\" button. An API token should automatically download and a prompt will also appear telling you which directory to put this token so python knows where find it. For MacOS users this location is \"~/.kaggle/kaggle.json\". Once you have done this modify the code below to download the dataset to the `data` folder distributed with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2fe231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log in to kaggle using your api token\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "#path relative to this notebook to put the data\n",
    "datadir = <yourcode>\n",
    "\n",
    "#name of the dataset on kaggle\n",
    "dataset = 'loveall/appliances-energy-prediction'\n",
    "\n",
    "#downlaod the data\n",
    "kaggle.api.dataset_download_files(dataset, path=datadir, unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7dfc6b",
   "metadata": {},
   "source": [
    "## Linear vs non-linear models\n",
    "Notice the non-linear model scored higher than the 3 regularized linear models. Is the non-linear model more appropriate because is scored higher on the testing set? How would you demonstrate the non-linear model is infact a more appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate some code to prove your point.\n",
    "<your code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4d054",
   "metadata": {},
   "source": [
    "## Advanced models\n",
    "Can you think of an even more appropriate model to use other than the deep-network and regularized models we used today? If the simplest model is the best model then argue why should we consider more complicated model to treat this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
