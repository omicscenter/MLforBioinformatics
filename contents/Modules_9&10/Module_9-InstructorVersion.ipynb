{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ace58e3-d3d7-4743-a14c-21a4f6e2aa21",
   "metadata": {},
   "source": [
    "# Module 9 - Leukemia project  week 1\n",
    "\n",
    "Today we will put everything we ahve learned so far to develop a complete end to end analysis. We will be working with gene expression data of leukemia patients aquired in 1999 and later [published in Science](https://doi.org/10.1126/science.286.5439.531). The paper demonstrated how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. The data was used to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL). Our excersice is to develop a model that discriminates between AML and ALL patients based only on the this gene expression data and compare our model with the published results of the 1999 paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103cd06",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's get all the requirements sorted before we move on to the excercise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4fec48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Requirements\n",
    "!pip install --upgrade ipykernel\n",
    "!pip install kaggle\n",
    "!pip install pandas\n",
    "!pip install tableone\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install seaborn\n",
    "\n",
    "# Globals\n",
    "seed = 1017\n",
    "\n",
    "#imports\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tableone import TableOne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "#magic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040877a2",
   "metadata": {},
   "source": [
    "## Loading the data via kaggle API\n",
    "The leukemia data set was sourced from kaggle. \n",
    "To download the data directly from [kaggle](kaggle.com) you will need to have a kaggle account. **It's free.** Once you create your kaggle account you can generate an API token. After you log in you should see a circular account icon in the upper-right of any kaggle page. Clicking on your account icon will open a right-sidebar where you can select \"Account\" to edit your account. Scroll down to the API section and click on the \"create new api token\" button. An API token should automatically download and a prompt will also appear telling you which directory to put this token so python knows where find it. For MacOS users this location is \"~/.kaggle/kaggle.json\". Once you have done this modify the code below to download the dataset to the `data` folder distributed with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log in to kaggle using your api token\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "#path relative to this notebook to put the data\n",
    "datadir = 'data'\n",
    "\n",
    "#name of the dataset on kaggle\n",
    "dataset = 'crawford/gene-expression'\n",
    "\n",
    "#downlaod the data\n",
    "kaggle.api.dataset_download_files(dataset, path=datadir, unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bd5b1-61c0-435a-b1ae-3047d3519bae",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "There are two datasets containing the initial (training, 38 samples) and independent (test, 34 samples) datasets used in the paper. These datasets contain measurements corresponding to ALL and AML samples from Bone Marrow and Peripheral Blood. Intensity values have been re-scaled such that overall intensities for each chip are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d23b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data as a pandas dataframe\n",
    "labels = pd.read_csv('data/actual.csv', index_col = 'patient')\n",
    "test = pd.read_csv('data/data_set_ALL_AML_independent.csv')\n",
    "train = pd.read_csv('data/data_set_ALL_AML_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366f740",
   "metadata": {},
   "source": [
    "***Task*** Use the head() function to quickly have a look at the training data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faeb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the head() function display th first few rows of the training data frame.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6cf3b",
   "metadata": {},
   "source": [
    "The testing set is formatted the same as the training set. Notice, that the gene description and accession numbers are given along with the count and outcome (call) for each patient. The patient outcomes are also provided in the file `actual.csv`. I think it will be more convientient to use the outcomes in this file and delete the 'call' columns in both the training and testing sets. ***Question: What are the observational units of interest?*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60583afe",
   "metadata": {},
   "source": [
    "## Formatting\n",
    "***Task*** Remove the 'call' columns from the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9c49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<--remove this for student verssion--> \n",
    "cols = [col for col in test.columns if 'call' in col]\n",
    "test = test.drop(cols, 1)\n",
    "\n",
    "cols = [col for col in train.columns if 'call' in col]\n",
    "train = train.drop(cols, 1)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d97cd6a",
   "metadata": {},
   "source": [
    "Let's consider what the observational unit should be. ***Task*** Format the data to have observations in rows and features in columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove this for stutent version \n",
    "train = train.T\n",
    "test = test.T\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f645b",
   "metadata": {},
   "source": [
    "We can also remove the gene bookkeeping data because we will not use it today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the gene bookkeeping data.\n",
    "train = train.drop(['Gene Description', 'Gene Accession Number'])\n",
    "test = test.drop(['Gene Description', 'Gene Accession Number'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954102d7",
   "metadata": {},
   "source": [
    "Now let's encode the outcomes for binary classification. We'll use Zeros for the ALL outcomes and Ones for AML. Remember the first 38 patients were partitioned for the training set the remainder are in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove for student version\n",
    "labels = labels.replace({'ALL':0,'AML':1})\n",
    "labels_train = labels[labels.index <= 38]\n",
    "labels_test = labels[labels.index > 38]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13caf6ea",
   "metadata": {},
   "source": [
    "### Treat missing data\n",
    "Before moving on to a table 1. Let's look for and treat any missing data this. Remember to check for values that don't make sense. I think replacing witht the mean value would be a reasonable imputation strategy.***Task*** check for unreasonable and missing values and impute them with the column mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove in student version\n",
    "\n",
    "#remove zeros\n",
    "#train = train.replace(0, np.nan)\n",
    "#test = test.replace(0, np.nan)\n",
    "\n",
    "#remove inf\n",
    "train = train.replace(np.inf, np.nan)\n",
    "test = test.replace(np.inf, np.nan)\n",
    "\n",
    "#impute with mean\n",
    "train = train.fillna(value = train.values.mean())\n",
    "test = test.fillna(value = test.values.mean())\n",
    "\n",
    "#KNN imputation -- don't bother\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74577963",
   "metadata": {},
   "source": [
    "***Question*** How would you go about visualizing the data we just formatted? Why can't I just make a table 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc3faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove from student version\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Do a PCA for all data (this should probably be done only with the training data)\n",
    "nPC=10\n",
    "df_all = train.append(test, ignore_index=True)\n",
    "X_all = preprocessing.StandardScaler().fit_transform(df_all)\n",
    "pca = PCA(n_components=nPC, random_state=seed)\n",
    "X_pca = pca.fit_transform(X_all)\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove from student version\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Do a PCA for all data (this should probably be done only with the training data)\n",
    "nPC=10\n",
    "pca = PCA(n_components=nPC, random_state=seed)\n",
    "\n",
    "#define standard scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "#fit pca on the training data\n",
    "#df_all = train.append(test, ignore_index=True)\n",
    "X_scaled = scaler.fit_transform(train)\n",
    "X_train_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "#transform the testing data using the fit scaler and pca objects\n",
    "X_test_pca = pca.transform(scaler.transform(test))\n",
    "\n",
    "print(X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d5a51",
   "metadata": {},
   "source": [
    "### Visualize Engineered Features\n",
    "Let's plot the feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3eb1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the principle component distributions\n",
    "for PC in range(nPC):\n",
    "    sns.kdeplot(data=X_train_pca[:, PC])\n",
    "    #X_pca[:,nPC].plot.kde(bw_method='scott') #use bw_method=.02 for a lower bandwidth gaussian representation\n",
    "    plt.legend([\"PC\" + str(PC)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b796e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove in student version \n",
    "\n",
    "# rescale the engineered features if neccessary\n",
    "X_train_pca = scaler.fit_transform(X_train_pca)\n",
    "X_test_pca = scaler.transform(X_test_pca)\n",
    "\n",
    "#plot the principle component distributions\n",
    "for PC in range(nPC):\n",
    "    sns.kdeplot(data=X_test_pca[:, PC])\n",
    "    #X_pca[:,nPC].plot.kde(bw_method='scott') #use bw_method=.02 for a lower bandwidth gaussian representation\n",
    "    plt.legend([\"PC\" + str(PC)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80c954",
   "metadata": {},
   "source": [
    "## Capstone Project\n",
    "We have chosen a more [challenging Acute Myeloid Leukemia dataset] (https://www.synapse.org/#!Synapse:syn2455683/wiki/64007) to test your ML skills. You will have to request access to this data through the link provided and follow directions to the 'request access' form. Originally DREAM challenge, this dataset contains multiple outcomes such as disease relapse and response to treatment. Both classificationa and regression tasks are possible. The data represents over 200 Leukemia patients and includes highly dimensional gemonics data in addition to clinical covariates. Your challenge is to develop models to address these outcomes completely End-to-End over the next few weeks. We will review your analysis in the final week of the course. We will periodically check in to make sure your analyses are goinig smoothly but for the mean time your analysis should generally follow these beats.\n",
    "+ ***Format*** the data with observations in rows and features in columns\n",
    "+ ***Manually exclude data*** like book keeping variables\n",
    "+ ***Normalize*** Data\n",
    "+ ***Treat missing*** Data\n",
    "+ Choose and employ a ***Data Partitioning*** scheme\n",
    "+ Do ***Feature selection*** for Clinical covariates and ***Dimension Reduction*** for omics data.\n",
    "+ ***Choose a model*** with the highest performance potential\n",
    "+ ***Tune your model***\n",
    "+ Caclulate ***Performance metrics***\n",
    "+ Report ***Model Predicitions***\n",
    "+ ***Interpret*** results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce14c9e",
   "metadata": {},
   "source": [
    "## Pathway Representation\n",
    "PCA is one of the most common dimension reduction schemes, however interpreting the biology from principle components is difficult. The PCs explain fluctuations in the data itself not necessarily the biological relationships at play. It would be better to choose a representation that is more convienient for interpretation in a clinical setting. Let us aggregate the data of related genes into a value that represents the pathways they participate in. Finding the predictive pathways may be more useful to explain the biology, design future invivio models, and drive treatment development.\n",
    "\n",
    "The goal is to assign a number to each pathway represented in our dataset and use these as the predictive features for each patient. For each pathway we will need to find all the genes in our dataset that participate in this pathway and describe how only these releated genes fluctuate. We'll use a PCA to do this and use the value of the first PC as the value for the pathway feature. We will then normalize this pathway representation data before moving on to feature selection.\n",
    "\n",
    "### Convert Affymetrix Gene IDs to Kegg IDs\n",
    "The source data has genes in the Affymetrix format. We would like to use  KEGG IDs because of the convientient pathway analysis python tools KEGG provides.\n",
    "\n",
    "[g:Convert](http://www.protocol-online.org/cgi-bin/prot/jump.cgi?ID=4145)\n",
    " is a gene identifier tool that allows conversion of genes, proteins, microarray probes, standard names, various database identifiers, etc. A mix of IDs of different types may be inserted to g:Convert. The user needs to select a target database; all input IDs will be converted into target database format. Input IDs that have no corresponding entry in target database will be displayed as N/A.\n",
    " \n",
    "Let's use the g:Convert web tool to convert the affymetrix IDs to KEGG enzyme IDs. Copy and paste the \"Gene Accession Number\" column in the training set CSV file into the Query box in g:Convert. Exclude the column header when you paste. Select \"Homo sapiens (Human)\" in the organism box and select \"KEGG_ENZYME\" in the target namespace box. Press the \"Run query\" button to convert the IDs. When the query finishes you can download the results as a CSV file and save it in the data folder.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the g:Convert file\n",
    "gprof = pd.read_csv('data/gProfiler_hsapiens.csv')\n",
    "gprof.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67dd25",
   "metadata": {},
   "source": [
    "The corresponding gene names can be found in the 'name' column.Notice, that many initial aliases map to the same gene name. For every unique gene name let's get the pathways they participate in. We'll follow closely the [KEGG tutorial](https://bioservices.readthedocs.io/en/master/kegg_tutorial.html) to show us how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18896749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install bioservices and import KEGG\n",
    "!pip install bioservices\n",
    "from bioservices.kegg import KEGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the kegg interface\n",
    "ntrface = KEGG() \n",
    "\n",
    "#Example search for pathways by gene name\n",
    "ntrface.get_pathway_by_gene(\"GAPDH\", \"hsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique genes in gprof dataframe\n",
    "genes = gprof['name'].unique()\n",
    "genes = genes[0:50] #limit to first 50 genes so it runs quickly\n",
    "len(genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#init dictionary to hold pathway to gene mapping\n",
    "pathway_dict = {}\n",
    "\n",
    "#loop over unique genes in the dataset\n",
    "for gene in genes:\n",
    "    #get pathways associated with gene\n",
    "    pathways = ntrface.get_pathway_by_gene(str(gene), \"hsa\")\n",
    "    #loop over associated pathways\n",
    "    try:\n",
    "        for pathway, desc in pathways.items():\n",
    "            #check if pathway is new\n",
    "            if not (pathway in pathway_dict):\n",
    "                #append new pathway and init gene list\n",
    "                pathway_dict[pathway] = [str(gene)]   \n",
    "            else:\n",
    "                # add gene to the known pathway's list of genes\n",
    "                pathway_dict[pathway].extend([str(gene)])\n",
    "    except AttributeError:\n",
    "        print(\"No pathways found for \" + str(gene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display dictionary as pandas Series so we can show the head\n",
    "display(pd.Series(pathway_dict).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c73b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove from student version\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Do a PCA for all data (this should probably be done only with the training data)\n",
    "nPC=1\n",
    "pca = PCA(n_components=nPC, random_state=seed)\n",
    "\n",
    "#define standard scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "#loop over pathways\n",
    "for pathway in pathway_dict:\n",
    "    #get Gene Accession names for every Gene in pathway\n",
    "    \n",
    "    #select rows representing the genes in the pathway\n",
    "    \n",
    "    #fit pca on the training data\n",
    "    #df_all = train.append(test, ignore_index=True)\n",
    "    X_scaled = scaler.fit_transform(train)\n",
    "    X_train_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    #transform the testing data using the fit scaler and pca objects\n",
    "    X_test_pca = pca.transform(scaler.transform(test))\n",
    "\n",
    "    print(X_train_pca.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
