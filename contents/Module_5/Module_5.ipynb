{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ace58e3-d3d7-4743-a14c-21a4f6e2aa21",
   "metadata": {},
   "source": [
    "# Module 5 - Getting to know your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c5bdf-2ad0-4cd2-89e5-c9b4a01b141b",
   "metadata": {},
   "source": [
    "Hello, and welcome to this introductory course in Machine Learning for Biomedical applications. To kick things off, I'd like to introduce you to [Weka](https://waikato.github.io/weka-wiki/downloading_weka/) one of the first Machine Learning platforms I used when I started out learning ML. Please take a minute and checkout [this great Weka video tutorial](https://waikato.github.io/weka-wiki/downloading_weka/)by Google. I think Weka is a great gateway platform that will undoubtedly take you a long way down these first steps into the ML ecosystem. In fact, Weka is still my goto platform for exploratory data analysis (EDA) which is the topic of today. Follow along the Weka example in the accompanying Lecture slides as we get to know the autism dataset distributed along with this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c0e58-1d2a-4bf7-892a-dbc2f216828f",
   "metadata": {},
   "source": [
    " ## What we learned about the autism dataset using Weka\n",
    " 1. We have 292 observations and 21 features\n",
    " 2. The authors have developed a model using Demographics and 10 Yes/No questions to score patients for autism.\n",
    " 3. Some features contain missing values.\n",
    " 4. Appears the \"relation\" categorical variable was filled in by hand and has typos.\n",
    " 5. Ethinicity for example is not uniformly sampled. Most patients are white.\n",
    " 6. Most patients are 4yrs old or 11 with few patients in between.\n",
    " 7. 2/3 of patients are Male.\n",
    " 8. We can train a model with 100% accuracy using only the 10 yes/no questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87be1a47-2f03-466e-85d3-1099f3bbd707",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "+ How would you interpret the value of author's score metric? \n",
    "+ What is the precision and sensitivity of the author's model?\n",
    "+ In our trained model using only the 10 yes/no questions, What score does a patient start off with before we start considering the answers to the 10 yes/no questions?\n",
    "+ What score would a new patient get if they answers yes for all 10 question? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bd5b1-61c0-435a-b1ae-3047d3519bae",
   "metadata": {},
   "source": [
    "## What question am I answering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f714fe5-0bba-4f7f-9adb-33a05946aca2",
   "metadata": {},
   "source": [
    "When getting to know new data try to think about the following questions:\n",
    "+ Why was the data collected? \n",
    "+ How the data was intended to be used?\n",
    "+ Who was the author? This can inform your understanding of the dataâ€™s purpose.\n",
    "+ What does the literature say?\n",
    "    + Cohorts and datasets are often well studied.\n",
    "    + What seem to be the important features, the clinical need they focus on.\n",
    "+ What do the values in the data represent?\n",
    "+ What are the values trying to represent?\n",
    "+ **What potential added value does this data have for the greater clinical need?** \n",
    "\n",
    "A lot of this preliminary work is going to be done by hand and driven by your understanding of the subject matter. It is easy to underestimate how much effort this will take especially if the subject is outside of your wheelhouse. Only after you complete this critical step should you begin to formulate your central question. Skipping this due diligence is a common pitfall leading poor or at the very least unimpactful science. So, it is not enough to ask\n",
    "*\"What relationship can I demonstrate with this data?\"*, rather, you should have a benchmark in mind you want to overcome __*because*__ you have this data.\n",
    "\n",
    "For example, let's say the clinical standard for diagnosing some disease is through a biopsy. A well accepted rubric exists that quantifies the level of pathology in the tissue sample based on some histological metrics. But, you have accompanying urine analysis data not utilized by the benchmark model and also have evidence suggesting it has some prognostic potential.\n",
    "\n",
    "You can:\n",
    "1. Demonstrate a gain in prognostic strength by adding urine markers to the benchmark model.\n",
    "2. Demonstrate the urine analysis is a more cost effective predictor than the invasive biopsy.\n",
    "3. Demonstrate a relationship between the urine markers and the underlying histological features to improve the decision makeing process dictating when a biopsy is performed.\n",
    "4. Inferr a causal relationship between the urine markers and histological features to make statements about the pathology progression.\n",
    "\n",
    "Each analysis will require the data to be formatted in its own way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bdc85a-2a23-41b6-915b-971891e31115",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917f0f0-0106-405d-8825-4f40107a8c07",
   "metadata": {},
   "source": [
    "In this excercise we will tidy up the 21 variables in Autism data. We will then build a predictive model to predict the binary YES/NO Autism diagnosis. To format we will:\n",
    "+ Load the autism data in .arff format to a pandas dataframe\n",
    "+ Check all categorical features have consistent annotations - ie. no typos\n",
    "+ Have no missing values\n",
    "+ Remove uninformative features\n",
    "+ Report a \"table 1\" of patient characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcec51e-4a59-4ce0-8abc-639dac60325a",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Let's begin by installing all the packages we'll need for this excercise. It's also good to set the random number seed first thing too so we can reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ab830-3757-45fe-a814-d2dbf394ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tableone\n",
    "!pip install matplotlib\n",
    "\n",
    "# Globals\n",
    "seed = 1017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d9a24-132d-4bcc-a75e-d9a6ae12e5e9",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Ideally, data should be loaded from a repostitory and acceced programmatically for the sake of transparancy and reproducibility. The best analyses are totally divorced from the source data. We don't have that here. Today we will be reading a data file distributed allong with this excercise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d2216-1491-4620-b1a6-77ede558737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "\n",
    "#Read the arff file with scipy.io.arrff.loadarrf which returns the data and metadata\n",
    "data, meta = arff.loadarff('data/Autism-Child-Data.arff')\n",
    "\n",
    "#the metadata contains information such as name and type of attributes along with value ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b289e8-933c-43c7-93b1-96318ea9cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the meta data\n",
    "display(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a993b24-dfce-49bd-9ce7-ad0dc95bee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the feature names and types can also be accessed as arrays\n",
    "display(meta.names())\n",
    "display(meta.types())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01fe44e-eeab-41a3-a447-92796e52a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data is loaded as a record array accessible by attribute names\n",
    "#but it might be more convenient to convert it to a pandas dataframe \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523ae75-4cc6-413c-9eb4-c8549388430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the data\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040853b-8a2b-4879-903a-7e872a997d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Those 'b' are python's way of displaying a bytes array and are not part of the data\n",
    "#They indicate you're treating a byte string, litterally a sequence of octets, which are ASCII cahracters.\n",
    "#They will appear only in nominal features. The numeric features \"age\" and \"result\" will not have them.\n",
    "#You can decode them using .decode(\"utf-8\")\n",
    "\n",
    "#Let's decode only the nominal features\n",
    "for i in range(len(meta.types())):\n",
    "    if meta.types()[i] == 'nominal':\n",
    "        df[meta.names()[i]] = df[meta.names()[i]].str.decode('utf-8')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992e1f2-7577-41e7-af58-6d5a0ced9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the data now\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faeede0-a9db-418c-a4d4-c33993fbd0c6",
   "metadata": {},
   "source": [
    "## Heal annotations\n",
    "\n",
    "Recall the 'Self' vs 'self' typo for the 'relation' variable.\n",
    "One easy fix is to convert all strings to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356450b-c5c6-4eaf-ac3d-14e772d06c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all nominal variable string elements to lowercase.\n",
    "for i in range(len(meta.types())):\n",
    "    if meta.types()[i] == 'nominal':\n",
    "        df[meta.names()[i]] = df[meta.names()[i]].str.lower()    \n",
    "\n",
    "#let's look at the data now\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223dd93-6cb0-4305-a5a6-715385fe6785",
   "metadata": {},
   "source": [
    "## Treat Missing\n",
    "\n",
    "Treating missing values generally falls into one of two strategies:\n",
    "1. A mask that globally indicates missing values.\n",
    "2. A sentinel value that indicates a missing entry.\n",
    "\n",
    "In the **mask** approach, it might be a same-sized Boolean array representation or use one bit to represent the local state of missing entry. This requires allocations of an additional Boolean array which adds memory and computational overhead. This \n",
    "\n",
    "In the **sentinel** value approach, a tag value is used for indicating the missing value, such as NaN (Not a Number), null or a special value which is part of the programming language. This limits the range of valid values that can be represented and may require extra logic in CPU and GPU arithmetic.\n",
    "\n",
    "Here we will use the sentinel approach but later we will use masks for image processing in the computer vision project. Pandas use sentinels to handle missing values, specifically Pandas use two already-existing Python null values:\n",
    "+ the python None object\n",
    "+ the floating-point NaN value\n",
    "\n",
    "None is a Python â€˜objectâ€™ data that is often used for missing data. Because it is a Python object, None cannot be used in any arbitrary NumPy/Pandas array, but only in arrays with data type â€˜objectâ€™. Notice the dtype in following integer numpy array created with a None element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16213957-49e6-45d5-9362-bb4e0cc856cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([[1,2,3],\n",
    "                [4,None,6]])\n",
    "arr1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b1188-deab-4992-acba-cb4a78e2d603",
   "metadata": {},
   "source": [
    "Because the array is of type object you will generally get an error if you perform aggregations like sum() or min()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e34b5-46ce-4960-9f52-bffd1f3ba1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to show error\n",
    "#np.sum(arr1, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c9d40-a542-4d0f-a915-8e3957d4803f",
   "metadata": {},
   "source": [
    "NaN is an acronym for 'Not a Number' and a special floating-point value in the standard IEEE floating-point representation. Notice the type of the array if initialize it with a NaN instead of None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4ee69-356d-410a-8ed8-109ca0b320d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = np.array([[1,2,3],\n",
    "                [4,np.nan,6]])\n",
    "arr2.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421f7bd-8898-4254-8a9e-6932183514d7",
   "metadata": {},
   "source": [
    "NaN is specifically a floating-point value; there is no such thing for integers, strings or other types. Also, be aware that regardless of the operations, the result of arithmetic with NaN will be another NaN. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f138070-4e0b-482d-bd50-330feff99b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggreating arrays with np.nan elements\n",
    "np.sum(arr2, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b096cbcd-448d-4b41-92ab-4beaf759e738",
   "metadata": {},
   "source": [
    "Pandas is built to handle the None and NaN nearly interchangeably, converting between them where appropriate. For types that donâ€™t have an available sentinel value, Pandas automatically type-casts when NaN values are present. Notice the difference in type in the following pandas series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb7a32-8426-419d-8901-3d39e8d59952",
   "metadata": {},
   "outputs": [],
   "source": [
    "iseries = pd.Series([0, 1, 2])\n",
    "aseries = pd.Series([\"a\", \"b\", \"c\"])\n",
    "display(iseries)\n",
    "display(aseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945deb7-7fab-4136-b176-d72b63af1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "iseries[1]= None\n",
    "aseries[1]= None\n",
    "display(iseries)\n",
    "display(aseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba24899-3d9d-42ce-8edb-f76a323616be",
   "metadata": {},
   "outputs": [],
   "source": [
    "iseries[2]= np.nan\n",
    "aseries[2]= np.nan\n",
    "display(iseries)\n",
    "display(aseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363fa2ff-dd33-4f37-a8c1-eb7b2a005eb7",
   "metadata": {},
   "source": [
    "Often missing values don't come in nice and clean with None or np.nan values. If we know what kind of characters used as missing values in the dataset, we can handle them while creating the DataFrame using na_values parameter `df = pd.read_csv(\"source.csv\", na_values = ['?', '&'])`\n",
    "\n",
    "However, since we already have the data loaded we will have to edit the dataframe ourselves. In our case we need to replace the '?' characters used to indicate missing data in the arff file with None. Pandas will atutomatically type-cast to NaNs for the numeric variables while keeping None for the categorical variables. This can be accomplished with the pandas replace() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758dff1-25a7-4fcf-8a82-4ff9bf347a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all \"?\" with None objects\n",
    "df = df.replace({ \"?\": None })   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892d3ac-1aa5-4a7a-a03d-570107fe0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now show summary stats for each feature\n",
    "for i in range(len(meta.types())):\n",
    "    display(\"--------------\")\n",
    "    display(df[[meta.names()[i]]].describe())\n",
    "    display(df[[meta.names()[i]]].value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ddfb8-5ba6-42f3-8d56-6352c46e89e6",
   "metadata": {},
   "source": [
    "### Counting the missing values\n",
    "Pandas has two useful methods for detecting missing values: isnull() and notnull() . Either one will return a Boolean mask over the data. For example:\n",
    "df.isnull() returns a Boolean same-sized DataFrame indicating if values are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9378401-25a1-495c-a363-2fbca51b03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the sum of missing values for each feature\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc83efed-7896-41a7-97f8-d9db9c3acffc",
   "metadata": {},
   "source": [
    "### Brutal approach\n",
    "You can use dropna() to remove missing values. You can specify to drop either rows or columns with missing data by specifying the axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6f324-9eea-45d4-8aba-6a3ea7dffc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.shape)\n",
    "display(df.dropna(axis=0, inplace=False).shape)\n",
    "display(df.dropna(axis=1, inplace=False).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28764d9f-40b5-4ffd-8263-d6697f7d5a18",
   "metadata": {},
   "source": [
    "### OK approach\n",
    "You can use fillna()to fill in missing values. It has extra arugments\n",
    "+ value: value to use to replace NaN\n",
    "+ method: method to use for replacing NaN. method='ffill' does the forward replacement. method='bfill' does the backword replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66b165-8d36-4418-a520-58592e05ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces missing values with values in the previous row\n",
    "display(df.fillna(axis=0, method='ffill', inplace=False).shape)\n",
    "# replaces missing values with a constant\n",
    "display(df.fillna(axis=0, value=0, inplace=False).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebed40-c955-4a4a-8944-48747b0342e4",
   "metadata": {},
   "source": [
    "### Better approach\n",
    "The other common replacement is to replace missing values with the mean for continuous variables or median for categorical ones. Median works for both so let's just go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35237b4c-5564-4656-8a33-2263cf5b14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(value=df.median(axis=1, skipna=True), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f965015-4c91-4dcf-8529-cca2a7c63658",
   "metadata": {},
   "source": [
    "## Remove uninformative variables\n",
    " If all observations have the same value for a particular feature then that feature cannot help discriminate among the observations. These variables should be removed. In later modules we will elaborate on this idea of informative variables when we discuss feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b92ba-85da-477a-b4d3-8633081a3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of unique values per feature\n",
    "for i in range(len(meta.names())):\n",
    "    display(meta.names()[i] +\" has \"+str(df[meta.names()[i]].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1342e-9246-4a9f-bebb-dc55fe728fe9",
   "metadata": {},
   "source": [
    "Looks like 'age_desc' has only one value. Let's remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524118ed-4bd0-41ac-a684-9afd2bfca1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove variables with one unique value\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() == 1:\n",
    "        #drop the column\n",
    "        print(col)\n",
    "        df.drop([col], axis=1, inplace=True)\n",
    "\n",
    "#display the remaining columns\n",
    "display(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30bae3-5071-4c05-804d-89b4b23bbbcc",
   "metadata": {},
   "source": [
    "Finally, we should remove the 'result' variable provided by the authors. We can't use this data to predict.\n",
    "\n",
    "**Challenge: Remove the 'result' column from the dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8358419-7cba-409b-a071-dfa4bd27901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write code here\n",
    "\n",
    "#this line should confirm the column has been removed \n",
    "display(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec309370-b258-4b4c-8a2f-69f50507708c",
   "metadata": {},
   "source": [
    "## Generating a Table 1\n",
    "Often in a biomedical paper the first table is a summary of the study population characteristics. Not only will your reviewers expect it, generating it is actually very useful to distill your understanding of the problem you propose to tackle. Fortunately there are many packages out there that will generate a table 1 for you. Here we will use the cleverly named 'tableone' package and stratify our summary based on the autism outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481bc205-8620-498a-a062-0b3173a0501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tableone import TableOne\n",
    "TableOne(df, groupby=\"Class/ASD\", pval=True, dip_test=True, normal_test=True, tukey_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcba1e1-2049-4061-975d-d6223a114c58",
   "metadata": {},
   "source": [
    "### Exploring the warnings\n",
    "Chi-squared test is used to see whether distributions of categorical variables differ from each another. Looks like the statistics for country, relation, and ethnicity variables are poor so we cannot say one way or another if the distributions are different.\n",
    "\n",
    "Normality test informs you which variables are not normally distributed. Often one would report the median and inter-quartile ranges instead of the mean and standard deviation for these variables.\n",
    "\n",
    "Tukey's rule is a method to find outliers among continuous variables. None were found here but we would visualize this this in a boxplot. To demonstrate, we'll create box plots for age.\n",
    "\n",
    "Hartigan's Dip Test is a test for multimodality. The test has suggested that the age  distribution may be multimodal. We'll plot the distributions too and have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719fcfe-eca2-4c23-8975-30ed5a94e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b13836-7273-444f-ad0d-bd0c6574eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plots\n",
    "df[['age']].boxplot(whis=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804deb0e-522b-43bd-bca4-5c28c19b3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributions\n",
    "df[['age']].dropna().plot.kde(figsize=[12,8])\n",
    "plt.legend(['Age (years)', 'result(score)'])\n",
    "plt.xlim([0,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500d9f49-b238-40dc-a167-0a7705b100a0",
   "metadata": {},
   "source": [
    "When the distribution has distinct humps like this then one might consider binning them to convert them into categorical features.\n",
    "\n",
    "**Challenge - Bin the age variable to categories 'child' and 'adolecent' with cut off at 8 years old and regenerate your Table 1** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a4bbe-f00e-4051-8acf-0606542adcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
