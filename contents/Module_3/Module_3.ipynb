{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module #3 - Data Science with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `pandas` `DataFrames` are always two-dimensional, rectangular arrays of data. So how is that different from a `numpy` array? Simply put, a `pandas` `DataFrame` is a \"reskinned\" or \"repurposed\" `numpy` array in that they are actually built on and around `numpy` arrays. However, there are a few points that make `DataFrames` more specialized for data science:\n",
    "\n",
    "- A `DataFrame` can contain different data types (numerics, strings, booleans) in different columns (though each column has to be homogenous), whereas a `numpy` array is entirely homogenous\n",
    "- A `DataFrame` can have named rows and columns, which makes inspecting data much more convenient\n",
    "- A `DataFrame` integrates well with several graphing/plotting packages, such as `matplotlib.pyplot`, `plotly`, `bokeh`, `seaborn`, and others to make data visualization easier\n",
    "- A `DataFrame` is always 2-dimensional, whereas a `numpy` array can be N-dimensional\n",
    "\n",
    "In conclusion, `numpy` arrays are extremely powerful objects for matrix math, whether that is basic linear algebra, image processing, or any other problem that can mathematically be expressed as an N-dimensional matrix. A `DataFrame` is a very specialized version of a matrix that is completely geared toward data science, summarizing and visualizing datasets, and extracting information from them. This means that each has a space of applications where it far outperforms the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing required packages\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `DataFrame` and accessing columns\n",
    "\n",
    "As hinted at during the previous module, `DataFrames` can be created manually from dictionaries (although in real life it's a lot more common to import them from csv files using `pd.read_csv()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_df = pd.DataFrame(data={\n",
    "    'col_one': range(1, 5),\n",
    "    'col_two': range(11, 15),\n",
    "    'col_three': range(21, 25)\n",
    "}, index = [\n",
    "    'row_one', 'row_two', 'row_three', 'row_four'\n",
    "])\n",
    "display(my_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then access individual columns or lists of columns by their name. When we use single brackets `[]`, the column is extracted as a `pandas` `Series` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_df['col_one'])\n",
    "display(type(my_df['col_one']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use double brackets `[[]]`, the column of interest is extracted as a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_df[['col_one']])\n",
    "display(type(my_df[['col_one']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge problem:** Can you write a line of code that will access both `col_one` **and** `col_two`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the index and column names\n",
    "\n",
    "If we want to access, or possibly overwrite, the index or column names of a `DataFrame`, we can do so by accessing a `DataFrame` object's `index` and `columns` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_df.index)\n",
    "display(my_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing elements by index\n",
    "\n",
    "We discussed before how to access columns. But how do we access individual cell values in a `DataFrame`? The `iloc[]` method accomplishes this by accessing the cell at a given row and column index. We can use single brackets `[]` to access the element directly and extract it as the type of object contained in the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_df.iloc[1,1])\n",
    "display(type(my_df.iloc[1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use double brackets `[[],[]]` to extract the element as a `DataFrame` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_df.iloc[[1],[1]])\n",
    "display(type(my_df.iloc[[1],[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use either notation with the `iloc[]` method to set individual cells to a new value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.iloc[1,1] = 0\n",
    "display(my_df)\n",
    "\n",
    "my_df.iloc[[1], [1]] = 12\n",
    "display(my_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing elements by name\n",
    "\n",
    "The `iloc[]` method accesses `DataFrame` elements by index, as shown above. The `loc[]` method does the same thing with row and column names, passed as strings to the method. Again, single brackets `[]` extract the cell as the data type contained within it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_df.loc['row_two','col_two'])\n",
    "display(type(my_df.loc['row_two', 'col_two']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And double brackets `[[],[]]` extract the element as a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_df.loc[['row_two'],['col_two']])\n",
    "display(type(my_df.loc[['row_two'],['col_two']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the `iloc[]` method, `loc[]` can be used to assign new values to a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.loc['row_two','col_two'] = 0\n",
    "display(my_df)\n",
    "\n",
    "my_df.loc[['row_two'], ['col_two']] = 12\n",
    "display(my_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge problem:** Can you write a line of code that accesses **all** of `col_two`? You can use either `loc[]` or `iloc[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering rows with logical gates\n",
    "\n",
    "Just like a `numpy` array, `DataFrames` can be filtered using a boolean, or logical gate. Here, we'll filter `my_df` to display only the rows that correspond to a certain logical filter. This can be used to filter a dataset based on the value of one of the variables within it. Note that this filters rows, not columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows by column value\n",
    "display(my_df[my_df['col_one'] == 1])\n",
    "display(my_df[my_df['col_one'] < 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discarding data\n",
    "\n",
    "Our dataset may contain null values, represented in `numpy` as `np.nan`. We can use the `dropna()` method to drop all rows that contain null values (we can also pass a subset of column names to `dropna()` to only filter null values from certain variables). Handling null values is a whole topic on its own that will be treated in depth later in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.iloc[0,0] = np.nan\n",
    "display(my_df)\n",
    "my_df.dropna(inplace = True)\n",
    "display(my_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge question:** On line 3 above, what does the argument `inplace = True` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to discard an entire column of data, we can do so using the `drop()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.drop(labels = 'col_one', axis = 1, inplace = True)\n",
    "display(my_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting values and sorting `DataFrames`\n",
    "\n",
    "The `count()` and `sort_values()` methods are very useful when we want to summarize or rank our data based on a certain metric. There are many other summary methods to calculate the mean, for example, or other statistical metrics. Refer to the `pandas` documentation for more information on existing methods and how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_df.count())\n",
    "display(my_df.sort_values(by = 'col_two', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging two `DataFrames`\n",
    "\n",
    "It is often necessary to merge overlapping data from two separate sources. This can be done with the `merge()` method as shown below. The `how` argument specifies the kind of merge we want, such as the union of the two datasets (`outer`) or the intersection (`inner`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df2 = pd.DataFrame(data={\n",
    "    'col_two': range(11, 15),\n",
    "    'col_four': range(31, 35),\n",
    "    'col_five': range(41, 45)\n",
    "}, index = [\n",
    "    'row_one', 'row_two', 'row_three', 'row_four'\n",
    "])\n",
    "display(my_df)\n",
    "display(my_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_merge = my_df.merge(my_df2, how = 'outer', on = 'col_two')\n",
    "display(my_df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_merge = my_df.merge(my_df2, how = 'inner', on = 'col_two')\n",
    "display(my_df_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple data visualization\n",
    "\n",
    "Data visualization is a huge subfield of data science and is invaluable in exploratory data analysis (will be discussed in-depth in later classes), and when presenting finished analyses. `Pandas` `DataFrames` integrate well with several popular plotting packages. Here, we use `matplotlib.pyplot` to create a basic scatter plot with the `plot()` method. Note that `plot()` can be used directly on a `DataFrame` object. Alternatively, it could also be used independently as:\n",
    "\n",
    "``` python\n",
    "plt.plot(x = my_df2.col_two, y = my_df2.col_four, kind = 'scatter')\n",
    "```\n",
    "\n",
    "We'll go further into graphing in the TV miniproject and the homework, but know that there are extensive plotting packages that create publication and presentation grade figures. Whenever we want to actually display a `pyplot` graph, we do so by calling `plt.show()`. We want to point out here that `matplotlib.pyplot` is a very ubiquitous but complicated library. We're not going to get into the details here, but refer to [this](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Matplotlib_Cheat_Sheet.pdf) for a cheat sheet if you want to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "display(my_df2)\n",
    "\n",
    "my_df2.plot(x = 'col_two', y = 'col_four', kind = 'scatter')\n",
    "plt.xlabel('column two')\n",
    "plt.ylabel('column four')\n",
    "plt.title('example graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge problem:** Can you change the code above to create a line plot instead of a scatter plot? Hint: The word **line** is key. For your convenience, the code from above is duplicated below so you can modify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_df2)\n",
    "\n",
    "my_df2.plot(x = 'col_two', y = 'col_four', kind = 'scatter')\n",
    "plt.xlabel('column two')\n",
    "plt.ylabel('column four')\n",
    "plt.title('example graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping data by a variable\n",
    "\n",
    "Grouping data by one of the variables in the dataset is done using the `groupby()` method. This allows us to summarize data more specifically, by introducing hierarchy. See the below example where we count values within groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df2.iloc[1,0] = 11\n",
    "my_df2.iloc[0,2] = np.nan\n",
    "display(my_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply applying `count()` will return the number of values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(my_df2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the data using `groupby()` first allows us to count within groups based on the values in `col_two`. Note how the `object.method()` notation can be chained to perform multiple tasks sequentially on the same data in one line (`object.method1().method2()` etc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df_grouped = my_df2.groupby('col_two').count()\n",
    "display(my_df_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to statistical testing\n",
    "\n",
    "The `scipy` package is a powerful tool to perform many statistical tests. Here we're just introducing a simple t-test, refer to the `scipy` documentation or Google for more advanced tests in the homework assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "display(my_df2)\n",
    "\n",
    "t, p = ttest_ind(my_df2.col_two, my_df2.col_four, equal_var = True)\n",
    "print(\"t statistic for the t-test between col_two and col_four: \" + str(t))\n",
    "print(\"p-value for the t-test between col_two and col_four: \" + str(p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
